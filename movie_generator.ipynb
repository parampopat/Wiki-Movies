{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie-generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parampopat/Wiki-Movies/blob/master/movie_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3-S9ljWKeX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf8a6a37-342b-4ebb-88da-d73607d1c144"
      },
      "source": [
        "\"\"\"\n",
        "__author__ = \"Param Popat\"\n",
        "__version__ = \"1\"\n",
        "__git__ = \"https://github.com/parampopat/\"\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n__author__ = \"Param Popat\"\\n__version__ = \"1\"\\n__git__ = \"https://github.com/parampopat/\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LszAQm_KKk4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a26f4604-9cd5-4f59-cea5-020c85551dd8"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4rLN6pTKxBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "73e2d89a-ac84-4253-e827-31e2b50206af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yky2-jL_Ko6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "movieNames = pd.read_csv(\"/content/drive/My Drive/Wiki-Movies/Plot_associations.csv\").iloc[:, 1].values\n",
        "raw_text = \"\"\n",
        "for filename in movieNames:\n",
        "    raw_text += open(\"/content/drive/My Drive/Wiki-Movies/Plot/\" + filename + '.txt').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jcICi28LNRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "44dc14f2-fcf8-4c4b-9ac7-7d28f17536aa"
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\n",
        "    \"Total Characters: \", n_chars)\n",
        "print(\n",
        "    \"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  84100\n",
            "Total Vocab:  54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15BYNpfLRVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffc6c3ba-2562-4c28-8667-a7cba5db641f"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  84000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ula84eObLUR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1902
        },
        "outputId": "a2be2b96-7b27-4e43-be6a-5d6f5bc67e2a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath = \"/content/drive/My Drive/Wiki-Movies/trained/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "84000/84000 [==============================] - 349s 4ms/step - loss: 2.9252\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.92522, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-01-2.9252-bigger.hdf5\n",
            "Epoch 2/50\n",
            "84000/84000 [==============================] - 343s 4ms/step - loss: 2.7228\n",
            "\n",
            "Epoch 00002: loss improved from 2.92522 to 2.72276, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-02-2.7228-bigger.hdf5\n",
            "Epoch 3/50\n",
            "84000/84000 [==============================] - 343s 4ms/step - loss: 2.6027\n",
            "\n",
            "Epoch 00003: loss improved from 2.72276 to 2.60267, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-03-2.6027-bigger.hdf5\n",
            "Epoch 4/50\n",
            "84000/84000 [==============================] - 343s 4ms/step - loss: 2.5051\n",
            "\n",
            "Epoch 00004: loss improved from 2.60267 to 2.50509, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-04-2.5051-bigger.hdf5\n",
            "Epoch 5/50\n",
            "84000/84000 [==============================] - 344s 4ms/step - loss: 2.4155\n",
            "\n",
            "Epoch 00005: loss improved from 2.50509 to 2.41553, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-05-2.4155-bigger.hdf5\n",
            "Epoch 6/50\n",
            "84000/84000 [==============================] - 342s 4ms/step - loss: 2.3251\n",
            "\n",
            "Epoch 00006: loss improved from 2.41553 to 2.32506, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-06-2.3251-bigger.hdf5\n",
            "Epoch 7/50\n",
            "84000/84000 [==============================] - 345s 4ms/step - loss: 2.2385\n",
            "\n",
            "Epoch 00007: loss improved from 2.32506 to 2.23851, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-07-2.2385-bigger.hdf5\n",
            "Epoch 8/50\n",
            "84000/84000 [==============================] - 348s 4ms/step - loss: 2.1623\n",
            "\n",
            "Epoch 00008: loss improved from 2.23851 to 2.16225, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-08-2.1623-bigger.hdf5\n",
            "Epoch 9/50\n",
            "84000/84000 [==============================] - 345s 4ms/step - loss: 2.0921\n",
            "\n",
            "Epoch 00009: loss improved from 2.16225 to 2.09210, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-09-2.0921-bigger.hdf5\n",
            "Epoch 10/50\n",
            "84000/84000 [==============================] - 344s 4ms/step - loss: 2.0341\n",
            "\n",
            "Epoch 00010: loss improved from 2.09210 to 2.03413, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-10-2.0341-bigger.hdf5\n",
            "Epoch 11/50\n",
            "84000/84000 [==============================] - 344s 4ms/step - loss: 1.9777\n",
            "\n",
            "Epoch 00011: loss improved from 2.03413 to 1.97773, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-11-1.9777-bigger.hdf5\n",
            "Epoch 12/50\n",
            "84000/84000 [==============================] - 342s 4ms/step - loss: 1.9220\n",
            "\n",
            "Epoch 00012: loss improved from 1.97773 to 1.92203, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-12-1.9220-bigger.hdf5\n",
            "Epoch 13/50\n",
            "84000/84000 [==============================] - 339s 4ms/step - loss: 1.8748\n",
            "\n",
            "Epoch 00013: loss improved from 1.92203 to 1.87476, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-13-1.8748-bigger.hdf5\n",
            "Epoch 14/50\n",
            "84000/84000 [==============================] - 342s 4ms/step - loss: 1.8328\n",
            "\n",
            "Epoch 00014: loss improved from 1.87476 to 1.83275, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-14-1.8328-bigger.hdf5\n",
            "Epoch 15/50\n",
            "84000/84000 [==============================] - 345s 4ms/step - loss: 1.7888\n",
            "\n",
            "Epoch 00015: loss improved from 1.83275 to 1.78885, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-15-1.7888-bigger.hdf5\n",
            "Epoch 16/50\n",
            "84000/84000 [==============================] - 346s 4ms/step - loss: 1.7534\n",
            "\n",
            "Epoch 00016: loss improved from 1.78885 to 1.75344, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-16-1.7534-bigger.hdf5\n",
            "Epoch 17/50\n",
            "84000/84000 [==============================] - 343s 4ms/step - loss: 1.7130\n",
            "\n",
            "Epoch 00017: loss improved from 1.75344 to 1.71305, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-17-1.7130-bigger.hdf5\n",
            "Epoch 18/50\n",
            "84000/84000 [==============================] - 340s 4ms/step - loss: 1.6828\n",
            "\n",
            "Epoch 00018: loss improved from 1.71305 to 1.68284, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-18-1.6828-bigger.hdf5\n",
            "Epoch 19/50\n",
            "84000/84000 [==============================] - 347s 4ms/step - loss: 1.6495\n",
            "\n",
            "Epoch 00019: loss improved from 1.68284 to 1.64953, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-19-1.6495-bigger.hdf5\n",
            "Epoch 20/50\n",
            "84000/84000 [==============================] - 342s 4ms/step - loss: 1.6164\n",
            "\n",
            "Epoch 00020: loss improved from 1.64953 to 1.61640, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-20-1.6164-bigger.hdf5\n",
            "Epoch 21/50\n",
            "84000/84000 [==============================] - 342s 4ms/step - loss: 1.5898\n",
            "\n",
            "Epoch 00021: loss improved from 1.61640 to 1.58978, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-21-1.5898-bigger.hdf5\n",
            "Epoch 22/50\n",
            "84000/84000 [==============================] - 340s 4ms/step - loss: 1.5628\n",
            "\n",
            "Epoch 00022: loss improved from 1.58978 to 1.56280, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-22-1.5628-bigger.hdf5\n",
            "Epoch 23/50\n",
            "84000/84000 [==============================] - 338s 4ms/step - loss: 1.5366\n",
            "\n",
            "Epoch 00023: loss improved from 1.56280 to 1.53660, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-23-1.5366-bigger.hdf5\n",
            "Epoch 24/50\n",
            "84000/84000 [==============================] - 337s 4ms/step - loss: 1.5131\n",
            "\n",
            "Epoch 00024: loss improved from 1.53660 to 1.51306, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-24-1.5131-bigger.hdf5\n",
            "Epoch 25/50\n",
            "84000/84000 [==============================] - 335s 4ms/step - loss: 1.4902\n",
            "\n",
            "Epoch 00025: loss improved from 1.51306 to 1.49016, saving model to /content/drive/My Drive/Wiki-Movies/trained/weights-improvement-25-1.4902-bigger.hdf5\n",
            "Epoch 26/50\n",
            "58496/84000 [===================>..........] - ETA: 1:44 - loss: 1.4573"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9tQyEUJLrtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = numpy.random.randint(0, len(dataX) - 1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}